{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import pickle\n",
    "import re\n",
    "import preprocessor as p\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score\n",
    "from IPython.display import Markdown as md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving posts \n",
    "Posts were retrieved from the following vk groups: \n",
    "* https://vk.com/itmomagistry \n",
    "* https://vk.com/careers_service_itmo \n",
    "* https://vk.com/itmoru \n",
    "* https://vk.com/itmo_exchange \n",
    "* https://m.vk.com/scicomm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts(group):\n",
    "\n",
    "    count = 100\n",
    "    offset = 0 \n",
    "    posts = []\n",
    "    retrieved_posts = 'yes'\n",
    "    \n",
    "    while retrieved_posts:\n",
    "\n",
    "        url = 'https://api.vk.com/method/wall.get'\n",
    "        params = {\n",
    "            'domain': group,\n",
    "            'filter': 'owner',\n",
    "            'count': count,\n",
    "            'offset': offset,\n",
    "            'access_token': '8e84f1f18e84f1f18e84f1f1db8eeb68e388e848e84f1f1d0bb32bd2b9ea45a67630377',\n",
    "            'v': 5.73\n",
    "        }\n",
    "\n",
    "        retrieved_posts = requests.get(url, params = params).json()['response']['items']\n",
    "\n",
    "        posts += retrieved_posts \n",
    "        offset += count   \n",
    "        time.sleep(0.5)\n",
    "            \n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = ['itmomagistry', 'careers_service_itmo', 'itmoru', 'itmo_exchange', 'scicomm']\n",
    "\n",
    "posts = []\n",
    "for group in groups:\n",
    "    posts += get_posts(group=group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump( posts, open( \"posts.p\", \"wb\" ) )\n",
    "posts = pickle.load( open( \"posts.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting posts into old (older than a year ago) and new (later than a year ago) ones. \n",
    "The old ones will be used for training. The new ones will be used for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(posts):\n",
    "    test = []\n",
    "    train = []\n",
    "    \n",
    "    year_ago = (datetime.now()-timedelta(days=365)).timestamp()\n",
    "    \n",
    "    for post in posts:\n",
    "        if post['date'] > year_ago:\n",
    "            test.append(post)\n",
    "        else:\n",
    "            train.append(post)\n",
    "            \n",
    "    return test, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_posts, train_posts = split_train_test(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "We have retrieved 13586 posts.<br>11105 from them were published before February 2019. These posts will be used for training<br>2481 were published after February 2019. These posts will be used for testing"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(f'We have retrieved {len(posts)} posts.<br>{len(train_posts)} from them were published before February 2019. These posts will be used for training<br>{len(test_posts)} were published after February 2019. These posts will be used for testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 3764,\n",
       " 'from_id': -54201931,\n",
       " 'owner_id': -54201931,\n",
       " 'date': 1549961199,\n",
       " 'marked_as_ads': 0,\n",
       " 'post_type': 'post',\n",
       " 'text': '‚ú®–î–µ–Ω—å —Ä–æ—Å—Å–∏–π—Å–∫–æ–π –Ω–∞—É–∫–∏.\\n–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø—Ä–∞–∑–¥–Ω–∏–∫ –æ—Ç–º–µ—á–∞–µ—Ç—Å—è –≤ —á–µ—Å—Ç—å –æ—Å–Ω–æ–≤–∞–Ω–∏—è –≤ –Ω–∞—à–µ–π —Å—Ç—Ä–∞–Ω–µ –ê–∫–∞–¥–µ–º–∏–∏ –Ω–∞—É–∫. –û–Ω–∞ –±—ã–ª–∞ —Å–æ–∑–¥–∞–Ω–∞ 295 –ª–µ—Ç –Ω–∞–∑–∞–¥.\\nüìù –ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ —á–∏—Ç–∞–π—Ç–µ –ø–æ —Å—Å—ã–ª–∫–µ: \\nhttps://www.1tv.ru/news/2019-02-08/360076-segodnya_den_rossiyskoy_nauki',\n",
       " 'attachments': [{'type': 'video',\n",
       "   'video': {'access_key': '27143b75f5296c628d',\n",
       "    'can_comment': 0,\n",
       "    'can_like': 1,\n",
       "    'can_repost': 1,\n",
       "    'can_subscribe': 1,\n",
       "    'can_add_to_faves': 1,\n",
       "    'can_add': 1,\n",
       "    'comments': 0,\n",
       "    'date': 1549961199,\n",
       "    'description': '–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø—Ä–∞–∑–¥–Ω–∏–∫ –æ—Ç–º–µ—á–∞–µ—Ç—Å—è –≤\\xa0—á–µ—Å—Ç—å –æ—Å–Ω–æ–≤–∞–Ω–∏—è –≤\\xa0–Ω–∞—à–µ–π —Å—Ç—Ä–∞–Ω–µ –ê–∫–∞–¥–µ–º–∏–∏ –Ω–∞—É–∫. –û–Ω–∞ –±—ã–ª–∞ —Å–æ–∑–¥–∞–Ω–∞ 295 –ª–µ—Ç –Ω–∞–∑–∞–¥.',\n",
       "    'duration': 0,\n",
       "    'photo_130': 'https://sun9-59.userapi.com/c852216/v852216335/b53c1/ADxFBZJ6jwo.jpg',\n",
       "    'photo_320': 'https://sun9-11.userapi.com/c852216/v852216335/b53c3/n1zeZST6cbU.jpg',\n",
       "    'photo_640': 'https://sun9-6.userapi.com/c852216/v852216335/b53c4/IFh5dSYKbTU.jpg',\n",
       "    'photo_800': 'https://sun9-6.userapi.com/c852216/v852216335/b53c4/IFh5dSYKbTU.jpg',\n",
       "    'id': 456239061,\n",
       "    'owner_id': -54201931,\n",
       "    'title': '–°–µ–≥–æ–¥–Ω—è\\xa0‚Äî –î–µ–Ω—å —Ä–æ—Å—Å–∏–π—Å–∫–æ–π –Ω–∞—É–∫–∏. –ù–æ–≤–æ—Å—Ç–∏. –ü–µ—Ä–≤—ã–π –∫–∞–Ω–∞–ª',\n",
       "    'track_code': 'video_7660c3f0xDoGOJ7-eAnwVlNcnKDmKhdqlelCy0pLNrdocoiaybD3EBg89f9_Dv1lYgOoldA',\n",
       "    'views': 4,\n",
       "    'local_views': 4,\n",
       "    'platform': '1tv'}}],\n",
       " 'post_source': {'type': 'vk'},\n",
       " 'comments': {'count': 0, 'can_post': 1, 'groups_can_post': True},\n",
       " 'likes': {'count': 1, 'user_likes': 0, 'can_like': 1, 'can_publish': 1},\n",
       " 'reposts': {'count': 0, 'user_reposted': 0},\n",
       " 'views': {'count': 627}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_posts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Retrieving text from posts and getting rid of duplicate posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_text(post):\n",
    "    if 'copy_history' in post:\n",
    "        text = post['copy_history'][0]['text']\n",
    "    else:\n",
    "        text = post['text']\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = {get_post_text(post) for post in test_posts}\n",
    "train_texts = {get_post_text(post) for post in train_posts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Found 622 duplicate posts from test set and 101 from train set."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(f'Found {len(train_posts) - len(train_texts)} duplicate posts from test set and {len(test_posts) - len(test_texts)} from train set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out posts that mention internships, scholarships, grants etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts_by_keywords(texts, words = {'—Å—Ç–∞–∂–∏—Ä–æ–≤–∫', '—Å—Ç–∞–∂–µ—Ä', '—Å—Ç–∞–∂—ë—Ä', 'scholarship', '—Å—Ç–∏–ø–µ–Ω–¥', '–≥—Ä–∞–Ω—Ç', 'intern', '–æ–±–º–µ–Ω'}):\n",
    "    return [text for text in texts if any(word in text for word in words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = get_texts_by_keywords(train_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dumping 500 of selected posts to excel sheet in order to label them manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(zip(train_texts[:500], [0]*500)).to_excel('train.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section we'll perform text classification.\n",
    "<br>**Input**: texts of posts mentionning internships, scholarships etc. with a corresponding class (labeled manually)\n",
    "<br>**Output**: one of the following classes\n",
    "\n",
    "\n",
    "#### Classes:\n",
    "* 0 - other\n",
    "* 1 - internships\n",
    "* 3 - scholarships, grants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving labeled data from excel sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('train_part2.xlsx')\n",
    "X_not_processed = df[0]\n",
    "y = df[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data:\n",
    "* removing urls, emojis and numbers\n",
    "* removing punctuation signs\n",
    "* lemmatization \n",
    "* removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, morph, stopwords):\n",
    "    p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.NUMBER)\n",
    "    text = p.clean(text)\n",
    "\n",
    "    words = [morph.parse(word)[0].normal_form for word in re.findall(r'\\w+', text)]\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "# russian_stopwords = get_stop_words('ru')\n",
    "russian_stopwords = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "**Before preprocessing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–∫–∞–∫ –ø—Ä–æ–π—Ç–∏ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º\\n\\n—Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–∞ –Ω–æ–≤—É—é —Ä–∞–±–æ—Ç—É ‚Äì –Ω–µ–ø—Ä–æ—Å—Ç–∞—è –∑–∞–¥–∞—á–∞. –∞ –µ—Å–ª–∏ –≤—ã –Ω–∞—à–ª–∏ –∏–¥–µ–∞–ª—å–Ω—É—é –¥–ª—è –≤–∞—Å –≤–∞–∫–∞–Ω—Å–∏—é –≤ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏, —Ç–æ –≤–∞—Å –∂–¥–µ—Ç –µ—â–µ –±–æ–ª–µ–µ —Å–µ—Ä—å–µ–∑–Ω–æ–µ –∏—Å–ø—ã—Ç–∞–Ω–∏–µ ‚Äì —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–∞ –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω–æ–º —è–∑—ã–∫–µ. —ç—Ç–æ –º–æ–∂–µ—Ç –≤—ã–∑—ã–≤–∞—Ç—å –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å, –¥–∞–∂–µ –µ—Å–ª–∏ –≤—ã –æ—Ç–ª–∏—á–Ω—ã–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª! [club21199653|kaplan international english] –ø–æ–¥–≥–æ—Ç–æ–≤–∏–ª —Å–æ–≤–µ—Ç—ã –¥–ª—è –≤–∞—Å, –∫–∞–∫ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å—Å—è –∫ —Ä–∞–∑–≥–æ–≤–æ—Ä—É —Å —Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª–µ–º –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ, –∏ –ª–µ–≥–∫–æ –ø–æ–ª—É—á–∏—Ç—å —Ä–∞–±–æ—Ç—É –º–µ—á—Ç—ã!\\n\\nhttp://kaplaninternational.com/rus/blog/how-to-interview-in-english/'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_not_processed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After preprocessing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–ø—Ä–æ–π—Ç–∏ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–æ–≤—ã–π —Ä–∞–±–æ—Ç–∞ –Ω–µ–ø—Ä–æ—Å—Ç–æ–π –∑–∞–¥–∞—á–∞ –Ω–∞–π—Ç–∏ –∏–¥–µ–∞–ª—å–Ω—ã–π –≤–∞–∫–∞–Ω—Å–∏—è –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–π –∫–æ–º–ø–∞–Ω–∏—è –∂–¥–∞—Ç—å –µ—â—ë —Å–µ—Ä—å—ë–∑–Ω—ã–π –∏—Å–ø—ã—Ç–∞–Ω–∏–µ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã–π —è–∑—ã–∫ —ç—Ç–æ –º–æ—á—å –≤—ã–∑—ã–≤–∞—Ç—å –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –æ—Ç–ª–∏—á–Ω—ã–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª club21199653 kaplan international english –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Å–æ–≤–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å—Å—è —Ä–∞–∑–≥–æ–≤–æ—Ä —Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª—å –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫ –ª–µ–≥–∫–æ –ø–æ–ª—É—á–∏—Ç—å —Ä–∞–±–æ—Ç–∞ –º–µ—á—Ç–∞'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(X_not_processed[0], morph, russian_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [preprocess(text, morph, russian_stopwords) for text in X_not_processed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting train data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the best classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizers = {'TfidfVectorizer':TfidfVectorizer(), \n",
    "               'CountVectorizer':CountVectorizer()\n",
    "              }\n",
    "classifiers = {'GradientBoostingClassifier':GradientBoostingClassifier(random_state = 0), \n",
    "               'RandomForestClassifier':RandomForestClassifier(random_state = 0), \n",
    "               'LinearSVC':LinearSVC(random_state = 0), \n",
    "               'MLPClassifier':MLPClassifier(random_state = 0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = {}\n",
    "for vectorizer_name, vectorizer in vectorizers.items():\n",
    "    scores = {}\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_val_vec = vectorizer.transform(X_val)\n",
    "    for classifier_name, classifier in classifiers.items():\n",
    "        classifier.fit(X_train_vec, y_train)\n",
    "        predictions = classifier.predict(X_val_vec)\n",
    "        scores[classifier_name] = f1_score(y_val, predictions, average = 'weighted')\n",
    "    f1_scores[vectorizer_name] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TfidfVectorizer</th>\n",
       "      <th>CountVectorizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GradientBoostingClassifier</th>\n",
       "      <td>0.673871</td>\n",
       "      <td>0.664512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.775001</td>\n",
       "      <td>0.767615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPClassifier</th>\n",
       "      <td>0.686452</td>\n",
       "      <td>0.726754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.638984</td>\n",
       "      <td>0.634742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            TfidfVectorizer  CountVectorizer\n",
       "GradientBoostingClassifier         0.673871         0.664512\n",
       "LinearSVC                          0.775001         0.767615\n",
       "MLPClassifier                      0.686452         0.726754\n",
       "RandomForestClassifier             0.638984         0.634742"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the most promissing results are achieved by the **GradientBoostingClassifier** together with **TfidfVectorizer**. \n",
    "<br>LinearSVC together with **TfidfVectorizer** gives a slightly worse result while being much simpler than GradientBoostingClassifier. The latter looks too sophisticated for our simple problem, so let's stick to the LinearSVC model and try to improve this model's performance by tuning hyperparameters using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_val_vec = vectorizer.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C':np.arange(0.01,100,10), 'max_iter':[1, 5, 10, 50, 100]}\n",
    "gs = GridSearchCV(LinearSVC(),param_grid,cv=5,return_train_score=True)\n",
    "gs.fit(X_train_vec,y_train)\n",
    "best_parameters = gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 70.01, 'max_iter': 5}\n"
     ]
    }
   ],
   "source": [
    "print(f'Best parameters: {best_parameters}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC(**best_parameters, random_state = 0)\n",
    "clf.fit(X_train_vec,y_train)\n",
    "predictions = clf.predict(X_val_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1_score = np.around(f1_score(y_val, predictions, average = 'weighted'), decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the model with tuned hyperparameters: 0.78\n"
     ]
    }
   ],
   "source": [
    "print(f'F1 score of the model with tuned hyperparameters: {best_f1_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4\n",
    "In this section we'll try to detect topics of posts about internships and about scholarships.\n",
    "<br>First, let's try to extract topics automatically using LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda(values, num_topics=3, num_words=5):\n",
    "    values = [preprocess(text, morph, russian_stopwords).split() for text in values]\n",
    "    dictionary = corpora.Dictionary(values)\n",
    "    corpus = [dictionary.doc2bow(text) for text in values]\n",
    "    ldamodel = LdaModel(corpus, num_topics, id2word=dictionary, passes=15)\n",
    "    topics = ldamodel.print_topics(num_words)\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_posts = {'internships': [post for post, c in zip(X_not_processed, predictions) if c == 1],\n",
    "                  'scholarships': [post for post, c in zip(X_not_processed, predictions) if c == 3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "internships \n",
      "\n",
      "0 (0, '0.012*\"—Ä–∞–±–æ—Ç–∞\" + 0.012*\"—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç\" + 0.012*\"–∏—Ç–º—ã–π\" + 0.010*\"—Å–≤–æ–π\" + 0.009*\"–∫–æ–º–ø–∞–Ω–∏—è\" + 0.009*\"—Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ\" + 0.009*\"—Å–∞–º—ã–π\" + 0.007*\"–º–æ—á—å\" + 0.007*\"—Å—Ç–∏–ø–µ–Ω–¥–∏—è\" + 0.007*\"—Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª—å\"')\n",
      "1 (1, '0.009*\"—Å—Ç–∞–∂–∏—Ä–æ–≤–∫–∞\" + 0.009*\"cup\" + 0.009*\"–ø–æ–ª—É—á–∞—Ç—å\" + 0.009*\"–ø–æ–ª—É—á–∏—Ç—å\" + 0.009*\"–∫–æ–º–ø–∞–Ω–∏—è\" + 0.009*\"–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\" + 0.009*\"–∏–Ω–∂–µ–Ω–µ—Ä\" + 0.007*\"—á–µ–º–ø–∏–æ–Ω–∞—Ç\" + 0.007*\"–∫–µ–π—Å\" + 0.007*\"–ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ\"')\n",
      "2 (2, '0.015*\"–∏—Ç–º—ã–π\" + 0.013*\"—Å—Ç—É–¥–µ–Ω—Ç\" + 0.013*\"—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç\" + 0.013*\"–∫–æ–Ω–∫—É—Ä—Å\" + 0.011*\"—É—á–∞—Å—Ç–∏–µ\" + 0.009*\"—Å—Ç–∏–ø–µ–Ω–¥–∏—è\" + 0.009*\"—É—á–∞—Å—Ç–Ω–∏–∫\" + 0.009*\"–Ω–∞—É—á–Ω—ã–π\" + 0.009*\"–ø—Ä–æ–µ–∫—Ç\" + 0.009*\"—à–∫–æ–ª–∞\"')\n",
      "\n",
      "scholarships \n",
      "\n",
      "0 (0, '0.014*\"–∫–æ–º–ø–∞–Ω–∏—è\" + 0.011*\"—Å–≤–æ–π\" + 0.011*\"–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ\" + 0.011*\"–≤–µ–¥—É—â–∏–π\" + 0.011*\"—Å—Ç–∞–∂–∏—Ä–æ–≤–∫–∞\" + 0.009*\"–∞–ø—Ä–µ–ª—å\" + 0.009*\"—Ä–∞–±–æ—Ç–∞\" + 0.009*\"–¥–µ–ª–æ–≤–æ–π\" + 0.009*\"—Å—Ç—É–¥–µ–Ω—Ç\" + 0.006*\"—ç—Ç–æ\"')\n",
      "1 (1, '0.011*\"–∫–æ–Ω–∫—É—Ä—Å\" + 0.011*\"—É—á–∞—Å—Ç–∏–µ\" + 0.011*\"—Å—Ç—É–¥–µ–Ω—Ç\" + 0.011*\"–ø—Ä–æ–µ–∫—Ç\" + 0.011*\"—Å—Ç–∞–∂–∏—Ä–æ–≤–∫–∞\" + 0.010*\"–∏—Ç–º—ã–π\" + 0.010*\"it\" + 0.008*\"–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–π\" + 0.008*\"–Ω–∞—É—á–Ω—ã–π\" + 0.007*\"–≥–æ–¥\"')\n",
      "2 (2, '0.014*\"—Å—Ç—É–¥–µ–Ω—Ç\" + 0.013*\"–∫—É—Ä—Å\" + 0.010*\"–∞–Ω–≥–ª–∏–π—Å–∫–∏–π\" + 0.010*\"–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–π\" + 0.010*\"—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç\" + 0.010*\"–º–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä–∞\" + 0.008*\"–Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–π\" + 0.008*\"–ø—Ä–æ–≥—Ä–∞–º–º–∞\" + 0.008*\"–¥–æ—Å—Ç—É–ø–Ω—ã–π\" + 0.007*\"—É—á–∞—Å—Ç–∏–µ\"')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for type, values in selected_posts.items():\n",
    "    print(type, '\\n')\n",
    "    topics = lda(values)\n",
    "    for i, topic in enumerate(topics):\n",
    "        print(i, topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, there doesn't seem to be any topics that could be easily extracted by the model (even if we tune number of topics). So let's use the good old string matching to find **the most wanted skills** in the intership posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at the most popular words in the posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('–∏—Ç–º—ã–π', 17), ('—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç', 14), ('—Å—Ç—É–¥–µ–Ω—Ç', 11), ('–∫–æ–º–ø–∞–Ω–∏—è', 11), ('—Å—Ç–∏–ø–µ–Ω–¥–∏—è', 11), ('—Ä–∞–±–æ—Ç–∞', 10), ('—Å–≤–æ–π', 9), ('—É—á–∞—Å—Ç–∏–µ', 8), ('itmo', 7), ('—Å—Ç–∞–∂–∏—Ä–æ–≤–∫–∞', 7), ('–≤–µ—Å—å', 7), ('–ø–æ–ª—É—á–∏—Ç—å', 7), ('–æ–ø—ã—Ç', 7), ('–º–æ—á—å', 6), ('–∫–æ–Ω–∫—É—Ä—Å', 6), ('—Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ', 6), ('–ø–æ–ª—É—á–∞—Ç—å', 6), ('—Å–∞–º—ã–π', 6), ('–∫–æ—Ç–æ—Ä—ã–π', 6), ('—Å—Ç–∞—Ç—å', 5), ('–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è', 5), ('–¥–∞—Ç—å', 5), ('—É—á–∞—Å—Ç–Ω–∏–∫', 5), ('–ø—Ä–æ–π—Ç–∏', 5), ('–ø—Ä–æ–µ–∫—Ç', 5), ('–≤–æ–ø—Ä–æ—Å', 5), ('cup', 5), ('–ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ', 5), ('—Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª—å', 5), ('–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–π', 4), ('–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ', 4), ('–∏–Ω–∂–µ–Ω–µ—Ä', 4), ('—à–∫–æ–ª–∞', 4), ('–≤–µ–¥—É—â–∏–π', 4), ('–≥—Ä–∞–Ω—Ç', 4), ('–∫–µ–π—Å', 4), ('—á–µ–º–ø–∏–æ–Ω–∞—Ç', 4), ('—Ä–æ—Å—Å–∏—è', 4), ('–Ω–∞—É—á–Ω—ã–π', 4), ('–≤–∞–∂–Ω—ã–π', 4), ('—ç—Ç–æ', 3), ('–æ–±—É—á–∞—é—â–∏–π—Å—è', 3), ('–≥–æ–¥', 3), ('–ø—Ä–∏–Ω—è—Ç—å', 3), ('–¥—Ä–æ–±–Ω—ã–π', 3), ('ulsee', 3), ('–ø—Ä–æ–≥—Ä–∞–º–º–∞', 3), ('–∞–±–∏—Ç—É—Ä–∏–µ–Ω—Ç', 3), ('–æ–±—É—á–µ–Ω–∏–µ', 3), ('–±–ª–æ–∫—á–µ–π–Ω', 3), ('–∫–∞–∂–¥—ã–π', 3), ('–∏–Ω—Å—Ç–∏—Ç—É—Ç', 3), ('—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è', 3), ('—è–≤–ª—è—Ç—å—Å—è', 3), ('–µ—â—ë', 3), ('–¥–µ–Ω—å', 3), ('–æ—Ç–∫—Ä—ã—Ç—ã–π', 3), ('–¥–≤–µ—Ä—å', 3), ('—Ä–µ—à–µ–Ω–∏–µ', 3), ('–∑–∞—è–≤–∫–∞', 3), ('–ø—Ä–∞–≤–æ', 3), ('–ø–æ–ª—É—á–µ–Ω–∏–µ', 3), ('–∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π', 3), ('–∏–º–µ—Ç—å', 3), ('–∑–∞–¥–∞—á–∞', 3), ('–∏–º–µ–Ω–Ω–æ', 3), ('–∫—Ä—É–ø–Ω—ã–π', 3), ('—É—á–∞—Å—Ç–≤–æ–≤–∞—Ç—å', 3), ('–∏–Ω—Ç–µ—Ä–≤—å—é', 3), ('—É—á—ë–Ω—ã–π', 3), ('–≤—ã–ø—É—Å–∫–Ω–∏–∫', 3), ('–Ω–µ–¥–µ–ª—è', 3), ('–º–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä–∞', 3), ('–≤–∞—à', 3), ('—Ä–µ–∑—é–º–µ', 3), ('–Ω–µ—Å–∫–æ–ª—å–∫–æ', 3), ('–ø—Ä–∏–≥–ª–∞—à–µ–Ω–∏–µ', 3), ('—Å—Ç–æ–∏—Ç—å', 3), ('—Ä–∞—Å—Å–∫–∞–∑–∞—Ç—å', 3), ('—á–µ–ª–æ–≤–µ–∫', 3), ('–∫–∞—Ñ–µ–¥—Ä–∞', 3), ('00', 3), ('–¥–∞–ª–µ–µ', 3), ('–∫–∞—Ä—Ç–∞', 3), ('–Ω–æ—è–±—Ä—å', 3), ('ieee', 3), ('—Ñ–æ—Ç–æ–Ω–∏–∫', 3), ('—Ñ–∏–Ω–∞–ª–∏—Å—Ç', 3), ('–∂–¥–∞—Ç—å', 2), ('–±–æ–ª—å—à–æ–π', 2), ('–∞–≤–≥—É—Å—Ç', 2), ('—Ä–µ–≥–∞—Ç–∞', 2), ('university', 2), ('–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–π', 2), ('–∑—Ä–µ–Ω–∏–µ', 2), ('—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è', 2), ('–∞–ª–≥–æ—Ä–∏—Ç–º', 2), ('–æ–±—Ä–∞–±–æ—Ç–∫–∞', 2), ('–ø—Ä–∏–≥–ª–∞—à–∞—Ç—å', 2), ('2', 2), ('goto', 2), ('–±–∏–æ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞', 2), ('–ª–µ–∫—Ü–∏—è', 2), ('–º–∞—Å—Ç–µ—Ä', 2), ('–∫–ª–∞—Å—Å', 2), ('–æ–∫—Ç—è–±—Ä—å', 2), ('–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å', 2), ('–∂–µ–ª–∞—é—â–∏–π', 2), ('–≤—ã–∏–≥—Ä–∞—Ç—å', 2), ('–ø–∞—Ä—Ç–Ω—ë—Ä', 2), ('–ø—É–Ω–∫—Ç', 2), ('–ø–æ–≤—ã—à–µ–Ω–Ω—ã–π', 2), ('–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π', 2), ('–¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ', 2), ('–ª–∏–±–æ', 2), ('–≤—Ä–µ–º—è', 2), ('–º–∞–≥–∏—Å—Ç—Ä–∞–Ω—Ç', 2), ('–ø–æ–ª–µ–∑–Ω—ã–π', 2), ('news', 2), ('–∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è', 2), ('–∂—É—Ä–Ω–∞–ª–∏—Å—Ç', 2), ('–º–∏—Ä', 2), ('–Ω–∞—à', 2), ('—á–∏—Ç–∞—Ç—å', 2), ('–Ω–∞–ø–æ–º–∏–Ω–∞—Ç—å', 2), ('–ø–æ–ª–µ–∑–Ω—ã–π—á–µ—Ç–≤–µ—Ä–≥', 2), ('—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç', 2), ('—Å–æ–±—ã—Ç–∏–µ', 2), ('–º–æ–ª–æ–¥–æ–π', 2), ('–ø–æ–¥–∞—Ç—å', 2), ('–æ—Ç–ø—Ä–∞–≤–∫–∞', 2), ('hr', 2), ('–æ—Ç–∫–ª–∏–∫', 2), ('–∞–¥—Ä–µ—Å', 2), ('–Ω–∞–ø–∏—Å–∞—Ç—å', 2), ('—Å–æ–ø—Ä–æ–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–π', 2), ('–ø–∏—Å—å–º–æ', 2), ('–ø–æ–∑–∏—Ü–∏—è', 2), ('—Å–æ—Ç—Ä—É–¥–Ω–∏–∫', 2), ('–ø–æ—á–µ–º—É', 2), ('–∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π', 2), ('—É—Å–ø–µ—Ç—å', 2), ('–ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∞', 2), ('–º–µ—Å—Ç–æ', 2), ('—Ö–æ—Ä–æ—à–∏–π', 2), ('—Ä–∞—Å—Å–∫–∞–∑', 2), ('–¥–æ–ª–∂–Ω—ã–π', 2), ('–∑–∞–±—ã—Ç—å', 2), ('–≥–ª–∞–≤–Ω–æ–µ', 2), ('–∫–æ–Ω—Ç–∞–∫—Ç', 2), ('—Å–∏–ª—å–Ω–æ', 2), ('–∏—é–Ω—å', 2), ('–∞—É–¥', 2), ('–æ–ª–∏–º–ø–∏–∞–¥–∞', 2), ('–∞—Å–ø–∏—Ä–∞–Ω—Ç—É—Ä–∞', 2), ('–≤—É–∑', 2), ('–ª–∞—É—Ä–µ–∞—Ç', 2), ('–ø—Ä–µ—Å—Ç–∏–∂–Ω—ã–π', 2), ('–æ–ø—Ç–æ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞', 2), ('–∞–ª–∞—É–¥–∏', 2), ('–º–∞—Ä–∏—è', 2), ('and', 2), ('10', 2), ('—Å—Å—ã–ª–∫–∞', 2), ('misis', 2), ('case', 2), ('–±–∞–ª–ª', 2), ('changellenge', 2), ('spb', 2), ('–±–∏–∑–Ω–µ—Å', 2), ('–ª–µ—Ç–æ–º', 1), ('–ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–µ', 1), ('–º–∞–π', 1), ('–≤–ø–µ—Ä–≤—ã–µ', 1), ('–ø–∞—Ä—É—Å–Ω—ã–π', 1), ('sail', 1), ('training', 1), ('international', 1), ('–∫–∞–Ω–¥–∏–¥–∞—Ç', 1), ('–ª—é–±–∞', 1), ('kronbars', 1), ('sailtraininginternational', 1), ('–æ–ø–ª–∞—á–∏–≤–∞—Ç—å', 1), ('—Å–∏–ª—å–Ω–µ–π—à–∏–π', 1), ('–∏–≥—Ä–æ–∫', 1), ('–º–∏—Ä–æ–≤–æ–π', 1), ('—Ä—ã–Ω–æ–∫', 1), ('—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ', 1), ('–ª–∏—Ü–æ', 1), ('–æ–±—ä—è–≤–ª—è—Ç—å', 1), ('–Ω–∞–±–æ—Ä', 1), ('—Å—Ç–∞–∂–µ—Ä—Å–∫–∏–π', 1), ('d', 1), ('–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ', 1), ('–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç', 1), ('–∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã–π', 1), ('–µ—Ä–∞—Å—Ç–æ–≤', 1), ('–ø–æ–ª–∏–Ω–∞', 1), ('polina', 1), ('com', 1)]\n"
     ]
    }
   ],
   "source": [
    "internship_texts = [preprocess(text, morph, russian_stopwords) for text in selected_posts['internships']]\n",
    "internship_words = [text.split() for text in internship_texts]\n",
    "internship_words = [item for sublist in internship_words for item in sublist]\n",
    "counter = collections.Counter(internship_words)\n",
    "print(counter.most_common(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's choose the words that correspond to some programmer's skills and are relatively popular in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_words = ['java', 'c', 'python', 'javascript', 'frontend', 'backend', 'linux', 'office',\n",
    "             'bi', 'data science', '–∞–Ω–∞–ª–∏–∑ –¥–∞—Ç—å', '–º–∞—à–∏–Ω–Ω—ã–π –æ–±—É—á–µ–Ω–∏–µ', 'machine learning', '–∞–Ω–∞–ª–∏—Ç–∏–∫', \n",
    "             '—Ç–µ—Å—Ç–∏—Ä–æ–≤—â–∏–∫', '—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ', '–∞–Ω–≥–ª–∏–π—Å–∫–∏–π', '–¥–∏–∑–∞–π–Ω', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', '—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫', '–∏–Ω–∂–µ–Ω–µ—Ä','1—Å']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_popular_skills(key_words, texts):\n",
    "    skills_dict = dict.fromkeys(key_words, 0)\n",
    "    for text in texts:\n",
    "        for word in key_words:\n",
    "            if word in text:\n",
    "                skills_dict[word] += 1\n",
    "    skills_dict = {skill: count for skill, count in sorted(skills_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return skills_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: 7\n",
      "bi: 1\n",
      "–∞–Ω–∞–ª–∏–∑ –¥–∞—Ç—å: 1\n",
      "–º–∞—à–∏–Ω–Ω—ã–π –æ–±—É—á–µ–Ω–∏–µ: 1\n",
      "–¥–∏–∑–∞–π–Ω: 1\n",
      "–∏–Ω–∂–µ–Ω–µ—Ä: 1\n",
      "java: 0\n",
      "python: 0\n",
      "javascript: 0\n",
      "frontend: 0\n",
      "backend: 0\n",
      "linux: 0\n",
      "office: 0\n",
      "data science: 0\n",
      "machine learning: 0\n",
      "–∞–Ω–∞–ª–∏—Ç–∏–∫: 0\n",
      "—Ç–µ—Å—Ç–∏—Ä–æ–≤—â–∏–∫: 0\n",
      "—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: 0\n",
      "–∞–Ω–≥–ª–∏–π—Å–∫–∏–π: 0\n",
      "—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞: 0\n",
      "—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫: 0\n",
      "1—Å: 0\n"
     ]
    }
   ],
   "source": [
    "skills_dict = get_most_popular_skills(key_words, internship_texts)\n",
    "for skill, count in skills_dict.items():\n",
    "    print(f'{skill}: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5\n",
    "## Test data\n",
    "Now let's take a look at the test set, i.e. texts published before Ferbuary 2019.\n",
    "\n",
    "#### First, let's use our classifier to extract texts that announce internships and scholarships. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts_orig = get_texts_by_keywords(test_texts)\n",
    "test_texts = [preprocess(text, morph, russian_stopwords) for text in test_texts_orig]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts_vec = vectorizer.transform(test_texts)\n",
    "test_predictions = clf.predict(test_texts_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's dump texts and our predictions to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(zip(test_texts_orig, test_predictions), \n",
    "             columns = ['text', 'class']).to_csv('part_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_internships = [post for post, c in zip(test_texts, test_predictions) if c == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the most wanted skills in 2019-2020?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: 54\n",
      "–∞–Ω–≥–ª–∏–π—Å–∫–∏–π: 16\n",
      "–∞–Ω–∞–ª–∏—Ç–∏–∫: 12\n",
      "—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞: 12\n",
      "bi: 11\n",
      "–∏–Ω–∂–µ–Ω–µ—Ä: 11\n",
      "—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: 10\n",
      "—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫: 10\n",
      "java: 7\n",
      "python: 7\n",
      "frontend: 5\n",
      "backend: 4\n",
      "data science: 3\n",
      "–¥–∏–∑–∞–π–Ω: 3\n",
      "javascript: 2\n",
      "linux: 2\n",
      "–∞–Ω–∞–ª–∏–∑ –¥–∞—Ç—å: 2\n",
      "–º–∞—à–∏–Ω–Ω—ã–π –æ–±—É—á–µ–Ω–∏–µ: 2\n",
      "—Ç–µ—Å—Ç–∏—Ä–æ–≤—â–∏–∫: 1\n",
      "office: 0\n",
      "machine learning: 0\n",
      "1—Å: 0\n"
     ]
    }
   ],
   "source": [
    "test_skills_dict = get_most_popular_skills(key_words, test_internships)\n",
    "for skill, count in test_skills_dict.items():\n",
    "    print(f'{skill}: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can we tell from the data?\n",
    "* Most of the companies were looking for developers, most of them for c and c++ languages.\n",
    "* Machine Learning, python, data science, business intelligence speciallists are much more likely to find an internship in 2020 than in the previous years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_from_text(key_words, text):\n",
    "    result = []\n",
    "    for word in key_words:\n",
    "        if word in text:\n",
    "            result.append(word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_internships_keywords = [get_keywords_from_text(key_words, text) for text in test_internships]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's dump internships texts and our predictions to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_internships_orig = [post for post, c in zip(test_texts_orig, test_predictions) if c == 1]\n",
    "pd.DataFrame(zip(test_internships_orig, test_internships_keywords), \n",
    "             columns = ['text', 'keywords']).to_csv('part_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
